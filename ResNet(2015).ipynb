{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqGfJ56QKNiA"
      },
      "source": [
        "# 1. Dataset Preprocessing\n",
        "데이터셋은 torchvision 패키지에서 제공하는 STL10 dataset을 이용하겠습니다.\n",
        "STL10 dataset은 10개의 label을 갖습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vj93PqwkJZvx"
      },
      "outputs": [],
      "source": [
        "# import package\n",
        "\n",
        "# model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# dataset and transformation\n",
        "from torchvision import datasets\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# display images\n",
        "from torchvision import utils\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# utils\n",
        "import numpy as np\n",
        "from torchinfo import summary\n",
        "import time\n",
        "import copy\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N11wVwf-KYnW",
        "outputId": "fa9e141e-2a30-4ee9-b865-aeb7a7e9bf35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "50000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "# specify the data path\n",
        "path2data = './data'\n",
        "\n",
        "# if not exists the path, make the directory\n",
        "if not os.path.exists(path2data):\n",
        "    os.mkdir(path2data)\n",
        "\n",
        "# load dataset\n",
        "train_ds = datasets.CIFAR100(path2data, train=True, download=True, transform=transforms.ToTensor())\n",
        "val_ds = datasets.CIFAR100(path2data, train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "print(len(train_ds))\n",
        "print(len(val_ds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh62zT5AKasI",
        "outputId": "e561c66c-d2e7-4994-c850-f86f412d07b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.50707513 0.48654887 0.44091785\n",
            "0.5087964 0.48739308 0.4419422\n"
          ]
        }
      ],
      "source": [
        "# To normalize the dataset, calculate the mean and std\n",
        "train_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in train_ds]\n",
        "train_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in train_ds]\n",
        "\n",
        "train_meanR = np.mean([m[0] for m in train_meanRGB])\n",
        "train_meanG = np.mean([m[1] for m in train_meanRGB])\n",
        "train_meanB = np.mean([m[2] for m in train_meanRGB])\n",
        "train_stdR = np.mean([s[0] for s in train_stdRGB])\n",
        "train_stdG = np.mean([s[1] for s in train_stdRGB])\n",
        "train_stdB = np.mean([s[2] for s in train_stdRGB])\n",
        "\n",
        "val_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in val_ds]\n",
        "val_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in val_ds]\n",
        "\n",
        "val_meanR = np.mean([m[0] for m in val_meanRGB])\n",
        "val_meanG = np.mean([m[1] for m in val_meanRGB])\n",
        "val_meanB = np.mean([m[2] for m in val_meanRGB])\n",
        "\n",
        "val_stdR = np.mean([s[0] for s in val_stdRGB])\n",
        "val_stdG = np.mean([s[1] for s in val_stdRGB])\n",
        "val_stdB = np.mean([s[2] for s in val_stdRGB])\n",
        "\n",
        "print(train_meanR, train_meanG, train_meanB)\n",
        "print(val_meanR, val_meanG, val_meanB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W6CjqePGKvgM"
      },
      "outputs": [],
      "source": [
        "# define the image transformation\n",
        "train_transformation = transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Resize(224),\n",
        "                        transforms.Normalize([train_meanR, train_meanG, train_meanB],[train_stdR, train_stdG, train_stdB]),\n",
        "                        transforms.RandomHorizontalFlip(),\n",
        "])\n",
        "\n",
        "val_transformation = transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Resize(224),\n",
        "                        transforms.Normalize([train_meanR, train_meanG, train_meanB],[train_stdR, train_stdG, train_stdB]),\n",
        "])\n",
        "\n",
        "# apply transforamtion\n",
        "train_ds.transform = train_transformation\n",
        "val_ds.transform = val_transformation\n",
        "\n",
        "# create DataLoader\n",
        "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "MbAYt9woKyLe",
        "outputId": "b78b462b-731c-4142-9f57-ee7dec3cb36c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n# display sample images \\ndef show(img, y=None, color=True):\\n    npimg = img.numpy()\\n    npimg_tr = np.transpose(npimg, (1,2,0))\\n    plt.imshow(npimg_tr)\\n    if y is not None:\\n        plt.title('labels :' + str(y))\\nnp.random.seed(1)\\ntorch.manual_seed(1)\\ngrid_size = 4\\nrnd_inds = np.random.randint(0, len(train_ds), grid_size)\\nprint('image indices:',rnd_inds)\\nx_grid = [train_ds[i][0] for i in rnd_inds]\\ny_grid = [train_ds[i][1] for i in rnd_inds]\\nx_grid = utils.make_grid(x_grid, nrow=grid_size, padding=2)\\nshow(x_grid, y_grid)\\n\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "# display sample images \n",
        "def show(img, y=None, color=True):\n",
        "    npimg = img.numpy()\n",
        "    npimg_tr = np.transpose(npimg, (1,2,0))\n",
        "    plt.imshow(npimg_tr)\n",
        "    if y is not None:\n",
        "        plt.title('labels :' + str(y))\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "grid_size = 4\n",
        "rnd_inds = np.random.randint(0, len(train_ds), grid_size)\n",
        "print('image indices:',rnd_inds)\n",
        "x_grid = [train_ds[i][0] for i in rnd_inds]\n",
        "y_grid = [train_ds[i][1] for i in rnd_inds]\n",
        "x_grid = utils.make_grid(x_grid, nrow=grid_size, padding=2)\n",
        "show(x_grid, y_grid)\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OABpHxY-RMfj"
      },
      "source": [
        "# 2. Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\nmodel = models.resnet101(weights=models.ResNet101_Weights.DEFAULT).to(device)\\nsummary(model, (1,3, 224, 224))\\n#model.fc\\n\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = models.resnet101(weights=models.ResNet101_Weights.DEFAULT).to(device)\n",
        "summary(model, (1,3, 224, 224))\n",
        "#model.fc\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_output_shape(module, img_dim):\n",
        "    # returns output shape\n",
        "    device = next(module.parameters()).device\n",
        "    dims = module(torch.rand(*(img_dim)).to(device)).data.shape\n",
        "    return dims\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # BatchNorm에 bias가 포함되어 있으므로, conv2d는 bias=False로 설정합니다.\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * self.expansion),\n",
        "        )\n",
        "\n",
        "        # identity mapping, input과 output의 feature map size, filter 수가 동일한 경우 사용.\n",
        "        self.shortcut = nn.Sequential()\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        # projection mapping using 1x1conv\n",
        "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * self.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.residual_function(x) + self.shortcut(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BottleNeck(BasicBlock):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BottleNeck,self).__init__(in_channels, out_channels, stride)\n",
        "\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * self.expansion),\n",
        "        )\n",
        "\n",
        "\n",
        "class IntrClassif(nn.Module):\n",
        "    # intermediate classifer head to be attached along the backbone\n",
        "    # Inpsired by MSDNet classifiers (from HAPI):\n",
        "    # https://github.com/kalviny/MSDNet-PyTorch/blob/master/models/msdnet.py\n",
        "\n",
        "    def __init__(self,input_shape, classes=100):\n",
        "        super(IntrClassif, self).__init__()\n",
        "        # index for the position in the backbone layer\n",
        "        # input shape to automatically size linear layer\n",
        "        # intermediate conv channels\n",
        "        #interChans = 128 # TODO reduce size for smaller nets\n",
        "        self.input_shape = input_shape\n",
        "        # conv, bnorm, relu 1\n",
        "        layers = nn.ModuleList()\n",
        "        self.conv1 = BasicBlock(input_shape[1],input_shape[1], stride=1)\n",
        "        layers.append(self.conv1)\n",
        "        self.conv2 = BasicBlock(input_shape[1],input_shape[1], stride=1)\n",
        "        layers.append(self.conv2)\n",
        "        self.layers = layers\n",
        "\n",
        "        self.linear_dim = int(torch.prod(torch.tensor(self._get_linear_size(layers))))\n",
        "        #print(f\"Classif @ {self.bb_index} linear dim: {self.linear_dim}\") #check linear dim\n",
        "        \n",
        "        # linear layer\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.linear_dim, classes)\n",
        "        )\n",
        "\n",
        "    def _get_linear_size(self, layers):\n",
        "        for layer in layers:\n",
        "            self.input_shape = get_output_shape(layer, self.input_shape)\n",
        "        return self.input_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x=layer(x)\n",
        "        return self.linear(x)\n",
        "\n",
        "class MultiExitResNet(nn.Module):\n",
        "    '''\n",
        "    five ee (total six exit) each of which consists of four convolutional layers (two residual blocks) and one FC layer. \n",
        "    The added five exits are located after the [18, 36, 54, 72, 90]th conv layers\n",
        "    train model 164 epochs using CIFAR-10 and CIFAR-100 datasets. \n",
        "    SGD optimizer with a learning rate of 0.1,a momentum of 0.9, and a weight decay of 10^-4. \n",
        "    The learning rate is decayed at epochs 81, 110, and 140 on a scale of 0.1.\n",
        "    '''\n",
        "    def __init__(self, num_classes=100, data_shape=[1,3,224,224],\n",
        "                 ptdmodel=None, exit_aft=[18, 36, 54, 72, 90]):\n",
        "        '''\n",
        "        data_shape: batch size must be 1. ex) [1,3,32,32]\n",
        "        '''\n",
        "        super(MultiExitResNet, self).__init__()\n",
        "\n",
        "        # NOTE structure:\n",
        "        # init conv -> exit1\n",
        "        # self.backbone\n",
        "        # self.end_layer (avg pool, flatten, linear)\n",
        "        self.num_classes=num_classes\n",
        "        self.ptdmodel = ptdmodel\n",
        "        self.exit_aft=exit_aft\n",
        "        self.exits = nn.ModuleList()\n",
        "        # weighting for each exit when summing loss\n",
        "        self.input_shape=data_shape #input data shape /batch, channels, height, width\n",
        "\n",
        "        self.exit_num=len(exit_aft)+1\n",
        "        self.fast_inference_mode = False\n",
        "        self.exit_loss_weights = [1/self.exit_num for _ in range(self.exit_num)] #for training need to match total exits_num\n",
        "        self.exit_threshold = torch.tensor([0.8], dtype=torch.float32) #for fast inference  #TODO: inference variable(not constant 0.8) need to make parameter\n",
        "        \n",
        "        self.init_conv = nn.Sequential(self.ptdmodel.conv1, self.ptdmodel.bn1, self.ptdmodel.relu, self.ptdmodel.maxpool)\n",
        "        self.backbone=nn.ModuleList()\n",
        "        for layer in [self.ptdmodel.layer1,self.ptdmodel.layer2,self.ptdmodel.layer3,self.ptdmodel.layer4]:\n",
        "            for block in layer:\n",
        "                self.backbone.append(block)\n",
        "        self.end_layers=nn.Sequential(self.ptdmodel.avgpool, nn.Flatten(), nn.Linear(in_features=self.ptdmodel.fc.in_features, out_features=num_classes))\n",
        "        self._build_exits()\n",
        "\n",
        "    def _build_exits(self): #adding early exits/branches\n",
        "        # TODO generalise exit placement for multi exit\n",
        "        # early exit 1\n",
        "        previous_shape=[] #len->5\n",
        "        tmp = self.init_conv(torch.rand(*(self.input_shape)).to(device))\n",
        "        eidx=0\n",
        "        for idx,module in enumerate(self.backbone):\n",
        "            tmp = module(tmp)\n",
        "            if(eidx<self.exit_num-1 and idx+1==(self.exit_aft[eidx]//3)):\n",
        "                previous_shape.append(tmp.data.shape)\n",
        "                eidx+=1\n",
        "        for i in range(self.exit_num-1):\n",
        "            ee = IntrClassif(previous_shape[i], self.num_classes)   #TODO \n",
        "            self.exits.append(ee)\n",
        "        #final exit\n",
        "        self.exits.append(self.end_layers)\n",
        "\n",
        "    @torch.jit.unused #decorator to skip jit comp\n",
        "    def _forward_training(self, x):\n",
        "        # TODO make jit compatible - not urgent\n",
        "        # NOTE broken because returning list()\n",
        "        res = []\n",
        "        y = self.init_conv(x)\n",
        "        #res.append(self.exits[0](y))\n",
        "        # compute remaining backbone layers\n",
        "        eidx=0\n",
        "        for idx,module in enumerate(self.backbone):\n",
        "            y = module(y)\n",
        "            if(eidx<self.exit_num-1 and idx+1==(self.exit_aft[eidx]//3)):\n",
        "                res.append(self.exits[eidx](y))\n",
        "                eidx+=1\n",
        "\n",
        "        # final exit\n",
        "        y = self.end_layers(y)\n",
        "        res.append(y)\n",
        "        return res\n",
        "\n",
        "    def exit_criterion_top1(self, x): #NOT for batch size > 1 (in inference mode)\n",
        "        with torch.no_grad():\n",
        "            pk = nn.functional.softmax(x, dim=-1)\n",
        "            #top1 = torch.max(pk)          #originally x*log(x)#TODO np.sum(pk*log(pk))\n",
        "            top1 = torch.log(pk)*pk\n",
        "            return top1 < self.exit_threshold\n",
        "\n",
        "    def forward(self, x):\n",
        "        #std forward function\n",
        "        if self.fast_inference_mode:\n",
        "            y = self.init_conv(x)\n",
        "            #res.append(self.exits[0](y))\n",
        "            # compute remaining backbone layers\n",
        "            eidx=0\n",
        "            for idx,module in enumerate(self.backbone):\n",
        "                y = module(y)\n",
        "                if(eidx<self.exit_num-1 and idx+1==(self.exit_aft[eidx]//3)):\n",
        "                    res = self.exits[eidx](y) #res not changed by exit criterion\n",
        "                    if self.exit_criterion_top1(res):\n",
        "                        return res\n",
        "                    eidx+=1\n",
        "            # final exit\n",
        "            res = self.end_layers(y)\n",
        "            return res\n",
        "        \n",
        "        else: # NOTE used for training\n",
        "            # calculate all exits\n",
        "            return self._forward_training(x)\n",
        "\n",
        "    def set_fast_inf_mode(self, mode=True):\n",
        "        if mode:\n",
        "            self.eval()\n",
        "        self.fast_inference_mode = mode\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model=MultiExitResNet(ptdmodel=models.resnet101(weights=models.ResNet101_Weights.DEFAULT).to(device)).to(device)\n",
        "#summary(m1, (1,3, 224, 224), device=device.type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPxxm3tIh9SC"
      },
      "source": [
        "# 3. Training part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fkMrR1RkZl9T"
      },
      "outputs": [],
      "source": [
        "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
        "opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9,weight_decay=0.0001)\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10)\n",
        "\n",
        "# function to get current lr\n",
        "def get_lr(opt):\n",
        "    for param_group in opt.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "# function to calculate metric per mini-batch\n",
        "def metric_batch(output, target):\n",
        "    pred = output.argmax(1, keepdim=True)\n",
        "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
        "    return corrects\n",
        "\n",
        "# function to calculate loss per mini-batch\n",
        "def loss_batch(loss_func, output_list, target, opt=None):\n",
        "    losses = [loss_func(output,target) for output in output_list]\n",
        "    metric_bs = [metric_batch(output, target) for output in output_list]\n",
        "    if opt is not None:\n",
        "        opt.zero_grad()\n",
        "        #backprop\n",
        "        for loss in losses[:-1]:\n",
        "            #ee losses need to keep graph\n",
        "            loss.backward(retain_graph=True)\n",
        "        #final loss, graph not required\n",
        "        losses[-1].backward()\n",
        "        opt.step()\n",
        "    return losses, metric_bs\n",
        "\n",
        "# function to calculate loss and metric per epoch\n",
        "def loss_epoch(model, loss_func, dataset_dl, opt=None):\n",
        "    running_loss = 0.0\n",
        "    running_metric = [0.0] * model.exit_num\n",
        "    len_data = len(dataset_dl.dataset)\n",
        "    tqdm_state = f'batch_training' if(opt is not None) else 'batch_validation'\n",
        "    for xb, yb in tqdm(dataset_dl, desc=tqdm_state, leave=False):\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        output_list = model(xb)\n",
        "\n",
        "        losses, metric_bs = loss_batch(loss_func, output_list, yb, opt)\n",
        "        for i, _ in enumerate(losses):\n",
        "            running_loss += losses[i].item()\n",
        "        running_metric = [sum(i) for i in zip(running_metric,metric_bs)]\n",
        "\n",
        "\n",
        "    loss = running_loss / len_data # float\n",
        "    metric = [100*i/len_data for i in running_metric] # float list[exit_num]\n",
        "\n",
        "    return loss, metric\n",
        "\n",
        "\n",
        "# function to start training\n",
        "def train_val(model, params):\n",
        "    num_epochs=params['num_epochs']\n",
        "    loss_func=params[\"loss_func\"]\n",
        "    opt=params[\"optimizer\"]\n",
        "    train_dl=params[\"train_dl\"]\n",
        "    val_dl=params[\"val_dl\"]\n",
        "    lr_scheduler=params[\"lr_scheduler\"]\n",
        "    path2weights=params[\"path2weights\"]\n",
        "\n",
        "    loss_history = {'train': [], 'val': []}\n",
        "    metric_history = {'train': [], 'val': []}\n",
        "\n",
        "    # # GPU out of memoty error\n",
        "    # best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        current_lr = get_lr(opt)\n",
        "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs-1, current_lr))\n",
        "\n",
        "        model.train()\n",
        "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, opt)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        metric_history['train'].append(train_metric)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl)\n",
        "        loss_history['val'].append(val_loss)\n",
        "        metric_history['val'].append(val_metric)\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            #best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(model.state_dict(), path2weights)\n",
        "            print('saved best model weights!')\n",
        "            print('Get best val_loss')\n",
        "\n",
        "        lr_scheduler.step(val_loss)\n",
        "\n",
        "        print(f'train loss: {train_loss:.6f}, val loss: {val_loss:.6f}, accuracy: {val_metric}, time: {(time.time()-start_time)/60:.4f} min')\n",
        "        print('-'*10)\n",
        "\n",
        "    #model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model, loss_history, metric_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jqDlPbgTrHa9"
      },
      "outputs": [],
      "source": [
        "# definc the training parameters\n",
        "params_train = {\n",
        "    'num_epochs':30,\n",
        "    'optimizer':opt,\n",
        "    'loss_func':loss_func,\n",
        "    'train_dl':train_dl,\n",
        "    'val_dl':val_dl,\n",
        "    'sanity_check':False,\n",
        "    'lr_scheduler':lr_scheduler,\n",
        "    'path2weights':'./models/weights.pt',\n",
        "}\n",
        "\n",
        "# create the directory that stores weights.pt\n",
        "def createFolder(directory):\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "    except OSError:\n",
        "        print('Error')\n",
        "createFolder('./models')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4qdtmCHvq09",
        "outputId": "a38cfe2e-aeac-4d11-e9f4-a19c34162077"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/29, current lr=0.01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "batch_training:   0%|          | 0/1563 [00:00<?, ?it/s]c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "batch_training:   3%|▎         | 54/1563 [01:54<37:19,  1.48s/it]  c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torch\\autograd\\__init__.py:251: UserWarning: Error detected in CudnnBatchNormBackward0. Traceback of forward call that caused the error:\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
            "    self.io_loop.start()\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\asyncio\\windows_events.py\", line 316, in run_forever\n",
            "    super().run_forever()\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
            "    self._run_once()\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
            "    handle._run()\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\asyncio\\events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
            "    await result\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"C:\\Users\\dongwoo\\AppData\\Local\\Temp\\ipykernel_22092\\2375338841.py\", line 2, in <module>\n",
            "    model, loss_hist, metric_hist = train_val(model, params_train)\n",
            "  File \"C:\\Users\\dongwoo\\AppData\\Local\\Temp\\ipykernel_22092\\759879329.py\", line 81, in train_val\n",
            "    train_loss, train_metric = loss_epoch(model, loss_func, train_dl, opt)\n",
            "  File \"C:\\Users\\dongwoo\\AppData\\Local\\Temp\\ipykernel_22092\\759879329.py\", line 42, in loss_epoch\n",
            "    output_list = model(xb)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"C:\\Users\\dongwoo\\AppData\\Local\\Temp\\ipykernel_22092\\2807793626.py\", line 197, in forward\n",
            "    return self._forward_training(x)\n",
            "  File \"C:\\Users\\dongwoo\\AppData\\Local\\Temp\\ipykernel_22092\\2807793626.py\", line 160, in _forward_training\n",
            "    y = module(y)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torchvision\\models\\resnet.py\", line 151, in forward\n",
            "    out = self.bn2(out)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\", line 171, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"c:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torch\\nn\\functional.py\", line 2478, in batch_norm\n",
            "    return torch.batch_norm(\n",
            " (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:119.)\n",
            "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "                                                                 \r"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Function 'CudnnBatchNormBackward0' returned nan values in its 0th output.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m model, loss_hist, metric_hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_train\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[8], line 81\u001b[0m, in \u001b[0;36mtrain_val\u001b[1;34m(model, params)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, current lr=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, num_epochs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, current_lr))\n\u001b[0;32m     80\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 81\u001b[0m train_loss, train_metric \u001b[38;5;241m=\u001b[39m \u001b[43mloss_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m loss_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m     83\u001b[0m metric_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_metric)\n",
            "Cell \u001b[1;32mIn[8], line 44\u001b[0m, in \u001b[0;36mloss_epoch\u001b[1;34m(model, loss_func, dataset_dl, opt)\u001b[0m\n\u001b[0;32m     41\u001b[0m yb \u001b[38;5;241m=\u001b[39m yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     42\u001b[0m output_list \u001b[38;5;241m=\u001b[39m model(xb)\n\u001b[1;32m---> 44\u001b[0m losses, metric_bs \u001b[38;5;241m=\u001b[39m \u001b[43mloss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(losses):\n\u001b[0;32m     46\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses[i]\u001b[38;5;241m.\u001b[39mitem()\n",
            "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mloss_batch\u001b[1;34m(loss_func, output_list, target, opt)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#backprop\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m losses[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#ee losses need to keep graph\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#final loss, graph not required\u001b[39;00m\n\u001b[0;32m     29\u001b[0m losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\dongwoo\\anaconda3\\envs\\_torch\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Function 'CudnnBatchNormBackward0' returned nan values in its 0th output."
          ]
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "model, loss_hist, metric_hist = train_val(model, params_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "wf_ZvkpYzS09",
        "outputId": "b578de2f-3ff3-41ca-a128-bd85f2ead1e4"
      },
      "outputs": [],
      "source": [
        "# Train-Validation Progress\n",
        "num_epochs=params_train[\"num_epochs\"]\n",
        "\n",
        "# plot loss progress\n",
        "plt.title(\"Train-Val Loss\")\n",
        "plt.plot(range(1,num_epochs+1),loss_hist[\"train\"],label=\"train\")\n",
        "plt.plot(range(1,num_epochs+1),loss_hist[\"val\"],label=\"val\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plot accuracy progress\n",
        "plt.title(\"Train-Val Accuracy\")\n",
        "plt.plot(range(1,num_epochs+1),metric_hist[\"train\"],label=\"train\")\n",
        "plt.plot(range(1,num_epochs+1),metric_hist[\"val\"],label=\"val\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owaCfqviJ-SI"
      },
      "outputs": [],
      "source": [
        "PATH='./models'\n",
        "torch.save(model.state_dict(), PATH+'bb.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPaf0TRMhAVyf2JaKY87/7U",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "ResNet(2015).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
